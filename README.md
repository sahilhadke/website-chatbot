# RAG-based Recursive Web Crawler

This project showcases a powerful web application built using Retrieval-Augmented Generation (RAG) architecture, combined with a recursive web crawler. The application is designed to crawl websites, extract valuable data, and enhance information retrieval capabilities using advanced language models. The implementation ensures that no data is missed during the crawling process.

## Installation

To set up the project locally, follow these steps:
1. Clone the repository:
```
git clone https://github.com/sahilhadke/website-rag.git
cd website-rag
```

2. Install the required dependencies:
```
pip install -r requirements.txt
```

3. Run the application:
```
streamlit run client.py
```

## Usage

* **Crawling**: Start the web crawler to recursively extract data from the specified domain.
* **Data Retrieval**: Use the search functionality to retrieve relevant information from the collected data using natural language queries.
* **Interactive Interface**: Utilize the web interface to interact with the application, view extracted data, and get contextually accurate responses.

## License

This project is licensed under the MIT License. See the LICENSE file for details.
